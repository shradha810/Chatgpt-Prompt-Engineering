# Chatgpt-Prompt-Engineering
https://learn.deeplearning.ai/courses/chatgpt-prompt-eng/

I experimented with the Llama2 7b-chat-hf model on Kaggle
Here are some key takeaways from this lesson:
Principle 1: Craft Clear & Secure Prompts
Clarity is King: Use clear and concise instructions to guide the model.
Delimiters are your friend: Employ quotes, brackets, and dashes to prevent "prompt injection" - unwanted user influence on the output. This ensures the model focuses on the task at hand, even if the user tries to deviate.
Structured Output Matters: Specify the desired output format (e.g., JSON, HTML) to streamline communication with the model.
Anticipate the Unexpected: Train the model to handle edge cases by providing examples and instructions on how to approach them.
Few-Shot Prompting: Get the model started with a few input-output examples to guide its understanding.
Principle 2: Patience is Key
Let the Model "Think": Break down complex tasks into manageable steps, allowing the model to process information thoroughly before jumping to conclusions.
By following these principles, you can unlock the true potential of large language models!

Zero-shot learning, few shot learning, summarisation, inferencing including sentiment analysis and generating topics discussed in any piece of text.
Temperature (0-1): towards 1 means more creative responses (model will output different outputs on multiple outputs)
